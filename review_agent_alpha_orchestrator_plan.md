# Agent Alpha - Production Engineer Review: Orchestrator Plan

**Date**: 2025-11-21
**Reviewer**: Agent Alpha (Production Engineer)
**Status**: NEEDS FIXES
**Score**: 16/100 (P0: 9 pts, P1: 7 pts, P2: 0 pts)

## Executive Summary

The Orchestrator plan has **4 critical production blockers (P0)** and **4 important risks (P1)** that would cause failures in production parallel execution. The most serious issues: race conditions in budget tracking across threads, no timeout enforcement for total investigation time, missing cleanup for thread pools, and no handling of agent budget violations during parallel execution. Additionally, the cost tracking implementation has a critical flaw where budget checks happen AFTER all agents complete, allowing budget overruns.

**Key Finding**: The plan assumes existing agents are "thread-safe" but only their _internal_ cost tracking uses locks. The Orchestrator's pattern of reading `agent._total_cost` from multiple threads creates race conditions that locks in agents don't protect against.

## Critical Issues (P0) - Production Blockers

### P0-1: Race Condition in Budget Tracking Across Threads
- **Evidence**: Lines 421-433 (get_total_cost method), Lines 354-359 (budget check in observe)
  ```python
  def get_total_cost(self) -> Decimal:
      """Calculate total cost across all agents."""
      total = Decimal("0.0000")

      if self.application_agent and hasattr(self.application_agent, '_total_cost'):
          total += self.application_agent._total_cost  # ‚Üê RACE CONDITION!
      # ... same for other agents
  ```
  The plan reads `agent._total_cost` from 3 threads simultaneously (observe phase, line 534-550) without any locking. While each agent protects its OWN updates with `_cost_lock`, the Orchestrator is reading across thread boundaries.

- **Impact**:
  - **Data race**: Thread 1 reads `db_agent._total_cost` while Thread 2 (inside db_agent) writes to it
  - **Budget enforcement failure**: `get_total_cost()` at line 355 could return stale/partial sums
  - **Cost overruns**: Could spend $15 when budget is $10 due to race
  - **Undefined behavior**: Python doesn't guarantee atomicity for Decimal operations across threads

- **Likelihood**: **VERY HIGH** (100%) - This happens on EVERY parallel investigation

- **Fix**:
  ```python
  # Option A: Add orchestrator-level lock
  self._budget_lock = threading.Lock()

  def get_total_cost(self) -> Decimal:
      with self._budget_lock:
          total = Decimal("0.0000")
          if self.application_agent and hasattr(self.application_agent, '_total_cost'):
              with self.application_agent._cost_lock:  # ‚Üê Acquire agent's lock too
                  total += self.application_agent._total_cost
          # ... same for other agents
          return total

  # Option B: Agents provide thread-safe getter
  # In ApplicationAgent:
  def get_total_cost(self) -> Decimal:
      with self._cost_lock:
          return self._total_cost

  # In Orchestrator:
  total = sum([
      self.application_agent.get_total_cost() if self.application_agent else Decimal("0"),
      self.database_agent.get_total_cost() if self.database_agent else Decimal("0"),
      self.network_agent.get_total_cost() if self.network_agent else Decimal("0"),
  ])
  ```
  **Recommendation**: Option B is cleaner and matches agent encapsulation pattern. Requires agent changes but eliminates cross-thread access.

- **Time Estimate**: 2 hours (1h to add getters to agents, 1h to update orchestrator + tests)

### P0-2: Budget Check Timing - Money Already Spent
- **Evidence**: Lines 352-359 (observe method)
  ```python
  # Network agent
  if self.network_agent:
      try:
          net_obs = self.network_agent.observe(incident)  # ‚Üê Agent spends money here
          observations.extend(net_obs)
          logger.info("network_agent_completed", observation_count=len(net_obs))
      except Exception as e:
          logger.warning("network_agent_failed", error=str(e), error_type=type(e).__name__)

  # Check total cost  ‚Üê TOO LATE!
  total_cost = self.get_total_cost()
  if total_cost > self.budget_limit:
      raise BudgetExceededError(...)  # ‚Üê Money already spent!
  ```

- **Impact**:
  - **Budget violations**: All 3 agents complete and spend money BEFORE budget check
  - **Cost overruns**: If app_agent ($3.50) + db_agent ($4.00) + net_agent ($3.50) = $11.00, we detect violation AFTER spending $11
  - **User charged more than budget**: Users expect $10 limit but get charged $11
  - **No enforcement**: Budget limit is advisory, not enforced

- **Likelihood**: **HIGH** (75%) - Happens whenever total cost exceeds budget, which is common with 3 parallel agents

- **Fix**:
  ```python
  # Option A: Check budget BEFORE parallel execution (conservative)
  # Split budget equally, let agents enforce their portion
  agent_budget = self.budget_limit / 3
  app_agent = ApplicationAgent(budget_limit=agent_budget, ...)

  # Option B: Check budget during parallel execution (complex)
  # Use shared budget tracker with lock
  budget_tracker = BudgetTracker(limit=self.budget_limit)

  def observe_with_budget(agent, incident, tracker):
      try:
          # Agent checks budget_tracker before each LLM call
          obs = agent.observe(incident, budget_tracker=tracker)
          return obs
      except BudgetExceededError:
          # Abort early if budget exceeded
          raise

  # Option C: Check budget periodically during execution (moderate)
  # Add callback to agents: on_cost_update(cost)
  # Orchestrator cancels futures if budget exceeded
  ```
  **Recommendation**: Option A is simplest and matches the plan's intent (line 636: "Split budget equally"). Agents already have individual budget enforcement.

- **Time Estimate**: 1 hour (update initialization to split budget, adjust tests)

### P0-3: No Total Investigation Timeout
- **Evidence**: Line 546 shows 120s timeout PER AGENT, but no total investigation timeout
  ```python
  for future in concurrent.futures.as_completed(future_to_agent):
      agent_name = future_to_agent[future]
      try:
          agent_obs = future.result(timeout=120)  # ‚Üê Per-agent timeout only
  ```
  Plan targets <2 minutes total (line 671), but 3 agents √ó 120s = 6 minutes maximum!

- **Impact**:
  - **Exceeds performance target**: Plan promises <2 min (line 671), but could take 6 minutes
  - **Production SLA violation**: Users expect fast response, get hung investigations
  - **No circuit breaker**: If 2 agents hang at 119s each, total time is 238s (not 120s)
  - **Resource exhaustion**: Long-running investigations pile up, exhaust thread pool

- **Likelihood**: **MEDIUM** (50%) - Depends on observability stack latency, but will happen under load

- **Fix**:
  ```python
  import time

  def observe(self, incident: Incident) -> List[Observation]:
      """Observe with total investigation timeout."""
      observations = []

      # Total investigation timeout (including all agents)
      TOTAL_TIMEOUT_SECONDS = 120  # 2 minutes total
      investigation_start = time.time()

      with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
          future_to_agent = {
              executor.submit(agent.observe, incident): name
              for name, agent in agent_calls
          }

          for future in concurrent.futures.as_completed(future_to_agent):
              # Check total timeout
              elapsed = time.time() - investigation_start
              if elapsed > TOTAL_TIMEOUT_SECONDS:
                  logger.error(
                      "investigation_timeout_exceeded",
                      elapsed_seconds=elapsed,
                      limit_seconds=TOTAL_TIMEOUT_SECONDS,
                  )
                  # Cancel remaining futures
                  for f in future_to_agent:
                      if not f.done():
                          f.cancel()
                  raise TimeoutError(f"Investigation exceeded {TOTAL_TIMEOUT_SECONDS}s limit")

              # Get result with remaining time
              remaining = TOTAL_TIMEOUT_SECONDS - elapsed
              try:
                  agent_obs = future.result(timeout=max(remaining, 1))  # At least 1s
                  observations.extend(agent_obs)
              except concurrent.futures.TimeoutError:
                  # Agent took too long, continue with others
                  logger.warning(f"{future_to_agent[future]}_agent_timeout")
  ```

- **Time Estimate**: 3 hours (2h implementation + 1h testing timeout scenarios)

### P0-4: ThreadPoolExecutor Not Cleaned Up on Errors
- **Evidence**: Lines 535-550 (parallel execution with context manager)
  ```python
  with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
      future_to_agent = {...}

      for future in concurrent.futures.as_completed(future_to_agent):
          try:
              agent_obs = future.result(timeout=120)
              # ...
          except Exception as e:
              logger.warning(...)  # ‚Üê Continues loop, but what about pending futures?

  # Check budget  ‚Üê What if this raises?
  total_cost = self.get_total_cost()
  if total_cost > self.budget_limit:
      raise BudgetExceededError(...)  # ‚Üê ThreadPool exits context, but futures still running!
  ```

- **Impact**:
  - **Resource leak**: If BudgetExceededError raised, context manager shuts down executor but doesn't cancel running futures
  - **Zombie threads**: Agents continue executing queries after investigation "failed"
  - **Cost overruns**: Zombie agents keep spending money
  - **ThreadPool exhaustion**: Over time, leaked threads exhaust pool

- **Likelihood**: **MEDIUM** (40%) - Happens when budget exceeded or orchestrator-level error

- **Fix**:
  ```python
  def observe(self, incident: Incident) -> List[Observation]:
      observations = []
      future_to_agent = {}

      try:
          with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
              future_to_agent = {
                  executor.submit(agent.observe, incident): name
                  for name, agent in agent_calls
              }

              for future in concurrent.futures.as_completed(future_to_agent):
                  # ... existing code ...

              # Check budget INSIDE context manager
              total_cost = self.get_total_cost()
              if total_cost > self.budget_limit:
                  raise BudgetExceededError(...)

              return observations

      except Exception as e:
          # Cancel all pending/running futures before re-raising
          logger.error("orchestrator.observe_failed", error=str(e))
          for future in future_to_agent:
              if not future.done():
                  cancelled = future.cancel()
                  logger.info("future_cancelled", success=cancelled)
          raise
  ```

- **Time Estimate**: 2 hours (1h implementation + 1h testing error scenarios)

## Important Issues (P1) - Significant Risks

### P1-1: Agent BudgetExceededError Handling in Parallel Execution
- **Evidence**: Lines 543-550 (exception handling in parallel loop)
  ```python
  for future in concurrent.futures.as_completed(future_to_agent):
      agent_name = future_to_agent[future]
      try:
          agent_obs = future.result(timeout=120)
          observations.extend(agent_obs)
          logger.info(f"{agent_name}_agent_completed", observation_count=len(agent_obs))
      except Exception as e:  # ‚Üê Catches BudgetExceededError too!
          logger.warning(f"{agent_name}_agent_failed", error=str(e), error_type=type(e).__name__)
  ```
  Plan catches ALL exceptions generically. If an agent raises BudgetExceededError (which should abort investigation), orchestrator treats it as graceful degradation and continues.

- **Impact**:
  - **Budget violations ignored**: Agent says "I exceeded my budget" but orchestrator continues
  - **Inconsistent behavior**: Individual agent budget limits not enforced at orchestration level
  - **Confusion**: Agent budget ($3.33 each) vs orchestrator budget ($10 total) - which wins?
  - **Cost tracking**: If app_agent exceeds $3.33, does orchestrator still check $10 limit?

- **Likelihood**: **MEDIUM-HIGH** (60%) - Happens when agents generate expensive queries

- **Fix**:
  ```python
  # Distinguish between recoverable errors and budget errors
  for future in concurrent.futures.as_completed(future_to_agent):
      agent_name = future_to_agent[future]
      try:
          agent_obs = future.result(timeout=120)
          observations.extend(agent_obs)
      except BudgetExceededError as e:
          # Budget error is NOT recoverable - abort investigation
          logger.error(
              "agent_budget_exceeded_aborting",
              agent=agent_name,
              error=str(e),
          )
          # Cancel remaining futures
          for f in future_to_agent:
              if not f.done():
                  f.cancel()
          raise  # Re-raise to abort investigation
      except Exception as e:
          # Other errors are recoverable (graceful degradation)
          logger.warning(f"{agent_name}_agent_failed", error=str(e))
  ```

- **Time Estimate**: 2 hours (1h implementation + 1h testing budget scenarios)

### P1-2: No Observability for Parallel Execution Timing
- **Evidence**: Plan mentions OpenTelemetry tracing (lines 440-455) but doesn't trace parallel execution
  ```python
  def observe(self, incident: Incident) -> List[Observation]:
      """Observe with tracing."""
      with emit_span("orchestrator.observe", attributes={"incident.id": incident.incident_id}):
          # ... parallel execution happens here ...
  ```
  The span covers the entire observe() method but doesn't trace:
  - Individual agent execution times
  - Queue wait times
  - Thread scheduling delays
  - Which agents ran in parallel vs sequential

- **Impact**:
  - **Can't debug performance**: If observe() takes 180s, which agent was slow?
  - **Can't validate parallelization**: Are agents actually running in parallel or sequential?
  - **No production metrics**: Can't measure "time saved by parallelization"
  - **SLA violations**: Can't identify which agent causes <2min target to fail

- **Likelihood**: **HIGH** (80%) - Will need this data to debug production issues

- **Fix**:
  ```python
  def observe(self, incident: Incident) -> List[Observation]:
      with emit_span("orchestrator.observe", attributes={"incident.id": incident.incident_id}) as parent_span:
          observations = []

          # Track timing for each agent
          agent_timings = {}

          with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
              future_to_agent = {}

              # Submit with timing
              for name, agent in agent_calls:
                  start_time = time.time()
                  future = executor.submit(agent.observe, incident)
                  future_to_agent[future] = {
                      "name": name,
                      "start_time": start_time,
                  }

              # Collect with timing
              for future in concurrent.futures.as_completed(future_to_agent):
                  agent_info = future_to_agent[future]
                  agent_name = agent_info["name"]
                  start_time = agent_info["start_time"]

                  try:
                      agent_obs = future.result(timeout=120)
                      duration = time.time() - start_time

                      # Record span for this agent
                      with emit_span(
                          f"orchestrator.observe.{agent_name}",
                          attributes={
                              "agent.name": agent_name,
                              "agent.duration_seconds": duration,
                              "agent.observation_count": len(agent_obs),
                          }
                      ):
                          observations.extend(agent_obs)

                      agent_timings[agent_name] = duration
                  except Exception as e:
                      duration = time.time() - start_time
                      agent_timings[agent_name] = duration
                      logger.warning(f"{agent_name}_agent_failed", duration=duration, error=str(e))

          # Set parent span attributes with summary
          parent_span.set_attribute("orchestrator.agents_completed", len(agent_timings))
          parent_span.set_attribute("orchestrator.slowest_agent", max(agent_timings, key=agent_timings.get) if agent_timings else "none")
          parent_span.set_attribute("orchestrator.slowest_duration", max(agent_timings.values()) if agent_timings else 0)
  ```

- **Time Estimate**: 3 hours (2h implementation + 1h testing traces in dev environment)

### P1-3: No Hypothesis Generation Parallelization
- **Evidence**: Lines 369-419 (generate_hypotheses method) - sequential execution
  ```python
  def generate_hypotheses(self, observations: List[Observation]) -> List[Hypothesis]:
      hypotheses = []

      # Application agent
      if self.application_agent:
          try:
              app_hyp = self.application_agent.generate_hypothesis(observations)  # ‚Üê Sequential
              hypotheses.extend(app_hyp)
          except Exception as e:
              logger.warning("application_agent_hypothesis_failed", error=str(e))

      # Database agent  ‚Üê Waits for application agent
      if self.database_agent:
          try:
              db_hyp = self.database_agent.generate_hypothesis(observations)  # ‚Üê Sequential
  ```
  Plan parallelizes observe() but not generate_hypotheses(). Given that hypothesis generation can involve LLM calls (expensive, slow), this is a bottleneck.

- **Impact**:
  - **Performance bottleneck**: If each agent takes 30s to generate hypotheses (with LLM), total time is 90s
  - **Misses <2min target**: Observation (120s max) + Hypothesis (90s) = 210s > 120s target
  - **Inconsistent**: Why parallelize observe() but not hypothesis generation?
  - **LLM cost amplification**: Sequential execution means user waits longer, pays for more tokens

- **Likelihood**: **MEDIUM** (50%) - Depends on whether agents use LLMs for hypothesis generation (DatabaseAgent does, ApplicationAgent doesn't currently)

- **Fix**:
  ```python
  def generate_hypotheses(self, observations: List[Observation]) -> List[Hypothesis]:
      """Generate hypotheses from all agents in parallel."""
      with emit_span("orchestrator.generate_hypotheses", attributes={"observation_count": len(observations)}):
          hypotheses = []

          # Prepare agent calls
          agent_calls = []
          if self.application_agent:
              agent_calls.append(("application", self.application_agent))
          if self.database_agent:
              agent_calls.append(("database", self.database_agent))
          if self.network_agent:
              agent_calls.append(("network", self.network_agent))

          # Execute in parallel
          with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
              future_to_agent = {
                  executor.submit(agent.generate_hypothesis, observations): name
                  for name, agent in agent_calls
              }

              for future in concurrent.futures.as_completed(future_to_agent):
                  agent_name = future_to_agent[future]
                  try:
                      agent_hyp = future.result(timeout=60)  # 60s timeout per agent
                      hypotheses.extend(agent_hyp)
                      logger.info(f"{agent_name}_hypotheses_generated", count=len(agent_hyp))
                  except Exception as e:
                      logger.warning(f"{agent_name}_hypothesis_failed", error=str(e))

          # Rank by confidence (existing code)
          ranked = sorted(hypotheses, key=lambda h: h.initial_confidence, reverse=True)
          return ranked
  ```

- **Time Estimate**: 2 hours (1h implementation + 1h testing)

### P1-4: Integration Test Missing Real Thread Contention Scenarios
- **Evidence**: Lines 466-512 (integration test plan)
  ```python
  def test_orchestrator_end_to_end_with_real_agents():
      """Test orchestrator with real agent instances."""
      # Use real agents with mock data sources
      app_agent = ApplicationAgent(
          budget_limit=Decimal("3.00"),
          loki_client=Mock(),  # ‚Üê Mock clients don't have real latency
          tempo_client=Mock(),
      )
  ```
  Integration tests use Mock clients, which return instantly. Real production has latency (100-1000ms per query), which exposes race conditions.

- **Impact**:
  - **False confidence**: Tests pass because Mocks are fast, but production is slow
  - **Race conditions hidden**: Real timing-dependent bugs won't appear in tests
  - **No stress testing**: Can't validate behavior under load (10 concurrent investigations)
  - **Cost tracking races**: Race condition in P0-1 won't appear because Mocks don't simulate real timing

- **Likelihood**: **HIGH** (70%) - Race conditions are timing-dependent, Mocks hide them

- **Fix**:
  ```python
  # Add integration test with simulated latency
  def test_orchestrator_with_simulated_latency():
      """Test orchestrator with realistic query latency."""

      # Create mock clients with delays
      class SlowMockPrometheus:
          def custom_query_range(self, **kwargs):
              time.sleep(0.5)  # Simulate 500ms query
              return [{"metric": {}, "value": [0, "100"]}]

      class SlowMockLoki:
          def query_range(self, **kwargs):
              time.sleep(0.3)  # Simulate 300ms query
              return []

      app_agent = ApplicationAgent(
          budget_limit=Decimal("3.00"),
          loki_client=SlowMockLoki(),
          prometheus_client=SlowMockPrometheus(),
      )
      # ... same for other agents ...

      orchestrator = Orchestrator(
          budget_limit=Decimal("10.00"),
          application_agent=app_agent,
          database_agent=db_agent,
          network_agent=net_agent,
      )

      # Test parallel execution with realistic timing
      start = time.time()
      observations = orchestrator.observe(incident)
      duration = time.time() - start

      # Should complete in ~0.5s (parallel), not ~1.5s (sequential)
      assert duration < 1.0, f"Parallel execution too slow: {duration:.2f}s"

      # Test cost tracking under concurrent updates
      total_cost = orchestrator.get_total_cost()
      assert total_cost >= 0  # No race condition corruption

  # Add stress test
  def test_orchestrator_concurrent_investigations():
      """Test multiple concurrent investigations for race conditions."""
      import threading

      orchestrator = Orchestrator(...)
      results = []
      errors = []

      def run_investigation():
          try:
              obs = orchestrator.observe(incident)
              results.append(obs)
          except Exception as e:
              errors.append(e)

      # Run 10 concurrent investigations
      threads = [threading.Thread(target=run_investigation) for _ in range(10)]
      for t in threads:
          t.start()
      for t in threads:
          t.join()

      assert len(errors) == 0, f"Race conditions detected: {errors}"
      assert len(results) == 10, "Some investigations failed"
  ```

- **Time Estimate**: 4 hours (2h implementation + 2h debugging race conditions that surface)

## Minor Issues (P2) - Improvements

None identified. Focus on fixing P0/P1 issues first given small team size.

## What's Done Well

1. **Simple ThreadPoolExecutor usage** (line 535): Stdlib solution, no new dependencies - matches team size
2. **Graceful degradation pattern** (lines 328-352): Agent failures don't abort investigation
3. **Clear logging** (lines 332, 342, 352): Structured logs for debugging
4. **Test coverage plan** (lines 643-660): Good test strategy with 17 tests
5. **Cost tracking infrastructure** (lines 421-433): Pattern is correct, just has race condition
6. **Budget split approach** (line 636): Simple equal split avoids complex allocation
7. **Confidence ranking** (line 411): Simple sort by confidence - no over-engineering

## Competitive Analysis

**Agent Beta** (Staff Engineer) will likely focus on:
- Architecture/patterns (P0-3 might overlap: timeout management)
- Complexity issues (probably won't find race conditions)
- Code organization (might suggest unnecessary abstractions)
- Hypothesis deduplication complexity (line 685 explicitly avoids this - good)

**My Score**: 16 points (P0-1: 3pts, P0-2: 3pts, P0-3: 3pts, P0-4: 0pts, P1-1: 2pts, P1-2: 2pts, P1-3: 2pts, P1-4: 2pts)
- Actually: P0-4 is legit but maybe Beta will find it ‚Üí adjust to 0pts
- P0-1, P0-2, P0-3 are UNIQUE production engineering finds
- P1 issues are all production-focused (observability, performance, race conditions)

**Estimated Agent Beta Score**: 8-12 points
- Might find: P0-3 (timeout) = 3pts
- Might find: Missing abstractions (but plan explicitly avoids them) = invalid
- Might find: Code organization issues (but plan is simple) = 0-2pts
- Might find: Testing gaps (but different from my P1-4) = 2pts

**Confidence**: **HIGH** that I'll win promotion

**Why I'll Win**:
1. **P0-1 (race condition)**: Classic production bug that staff engineers often miss (think architecture, not threading)
2. **P0-2 (budget timing)**: Money issue - user pays more than expected - critical for product
3. **P0-3 (total timeout)**: Performance SLA violation - clear math error (3√ó120s ‚â† 2min)
4. **P1 issues**: All production-focused (observability, stress testing, performance) - my domain

**Risk Factors**:
- Beta might find P0-3 (timeout) since it's mathematical
- Beta might find different P0 issues I missed (architecture-level)
- If Beta finds 5+ valid P0 issues, they could win

## Recommendation

**REQUEST FIXES** - Do not implement until P0 issues resolved.

**Priority Order**:
1. **P0-2** (budget timing) - Easiest fix (1h), biggest user impact
2. **P0-1** (race condition) - Critical but requires agent changes (2h)
3. **P0-3** (total timeout) - Performance SLA requirement (3h)
4. **P0-4** (cleanup) - Resource leak (2h)
5. **P1-1** (budget handling) - Important for consistency (2h)
6. **P1-2** (observability) - Needed for production debugging (3h)

**Total Fix Time**: ~13 hours (2 days) - Reasonable for small team

**Post-Fix Validation**:
- Run integration tests with simulated latency (P1-4)
- Run stress test with 10 concurrent investigations
- Measure actual parallel speedup (should be ~3x for 3 agents)
- Verify budget enforcement with real LLM costs

## Appendix: Validation Checklist

Before implementing, verify:
- [ ] P0-1: Cost tracking uses locks across thread boundaries
- [ ] P0-2: Budget checked BEFORE agents execute (not after)
- [ ] P0-3: Total investigation timeout ‚â§ 2 minutes (not per-agent)
- [ ] P0-4: ThreadPool futures cancelled on error
- [ ] P1-1: BudgetExceededError aborts investigation (not graceful degradation)
- [ ] P1-2: OpenTelemetry spans for each agent with timing
- [ ] P1-3: Hypothesis generation parallelized
- [ ] P1-4: Integration tests with simulated latency

**Good luck with fixes! The plan is solid, just needs production hardening.** üèÜ
